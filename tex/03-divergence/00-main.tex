\section{Divergence minimization}

Now, let us analyze some theoretical properties of GANs. For convenience, we will denote $p_{\theta}(\bm{x})$ 
to be the distribution whose samples are generated by first sampling $\bm{z} \sim \calN(0,I)$ and then 
returning the sample $G_{\theta}(\bm{z})$. With this notation, we may compactly express the discriminatorâ€™s loss as

\begin{equation} \label{eq:12}
    L_{D}(\phi;\theta) = - \E_{\bm{x} \sim p_{\text{data}}(\bm{x})} [\log D_{\phi}(\bm{x})] - \E_{\bm{x}\sim p_{\theta}(\bm{x})}[\log (1 - D_{\phi}(\bm{x}))]
\end{equation}

\begin{enumerate}[label=(\alph*)]
    \item \input{03-divergence/01-minimized}

    \item \input{03-divergence/02-estimate}

    \item \input{03-divergence/03-generator-loss}

    \item \input{03-divergence/04-implicit}
    
\end{enumerate}